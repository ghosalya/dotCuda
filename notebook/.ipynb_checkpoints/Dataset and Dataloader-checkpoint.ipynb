{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data folder structure\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['v2_mscoco_val2014_complementary_pairs.json',\n",
       " 'v2_Questions_Val_mscoco.zip',\n",
       " 'v2_mscoco_val2014_annotations.json',\n",
       " 'v2_OpenEnded_mscoco_val2014_questions.json',\n",
       " 'v2_mscoco_train2014_annotations.json',\n",
       " 'annotations',\n",
       " 'train2014',\n",
       " 'v2_OpenEnded_mscoco_train2014_questions.json',\n",
       " 'v2_Questions_Train_mscoco.zip',\n",
       " 'v2_mscoco_train2014_complementary_pairs.json',\n",
       " 'val2014']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDir ='../../data2'\n",
    "os.listdir(dataDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes on data folder\n",
    "--\n",
    "- `annotations`: not required, only for image captioning\n",
    "- `train2014` and `val2014`: img folders with format e.g. `COCO_val2014_000000059710.jpg`\n",
    "- `v2_Questions_Train_mscoco.zip` and `v2_Questions_Val_mscoco.zip`: unzip into respective questions `json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from vqaTools.vqa import VQA\n",
    "\n",
    "dataDir ='../../data2'\n",
    "versionType ='v2_' # this should be '' when using VQA v2.0 dataset\n",
    "taskType ='OpenEnded' # 'OpenEnded' only for v2.0. 'OpenEnded' or 'MultipleChoice' for v1.0\n",
    "dataType ='mscoco'  # 'mscoco' only for v1.0. 'mscoco' for real and 'abstract_v002' for abstract for v1.0.\n",
    "dataSubType ='train2014'\n",
    "annFile ='{}/{}{}_{}_annotations.json'.format(dataDir, versionType, dataType, dataSubType)\n",
    "quesFile ='{}/{}{}_{}_{}_questions.json'.format(dataDir, versionType, taskType, dataType, dataSubType)\n",
    "imgDir = '{}/{}/'.format(dataDir, dataSubType)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading VQA annotations and questions into memory...\n",
      "0:00:10.382276\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# initialize VQA api for QA annotations\n",
    "vqa = VQA(annFile, quesFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'answer_type': 'other',\n",
       "  'multiple_choice_answer': 'orange',\n",
       "  'answers': [{'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 1},\n",
       "   {'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 2},\n",
       "   {'answer': 'orange', 'answer_confidence': 'maybe', 'answer_id': 3},\n",
       "   {'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 4},\n",
       "   {'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 5},\n",
       "   {'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 6},\n",
       "   {'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 7},\n",
       "   {'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 8},\n",
       "   {'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 9},\n",
       "   {'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 10}],\n",
       "  'image_id': 458752,\n",
       "  'question_type': 'what color is the',\n",
       "  'question_id': 458752002}]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "image = vqa.qqa[458752000]['image_id']\n",
    "qa = vqa.loadQA(458752002)\n",
    "# qa['multiple_choice_answer']\n",
    "# [answer for answer in qa[0]['answers']\n",
    "\n",
    "qa\n",
    " \n",
    "# for a in qa[0]['answers']:\n",
    "#     print(a['answer'])\n",
    "# ans_list = [a['answer'] for a in qa[0]['answers']]\n",
    "\n",
    "\n",
    "# len(vqa.dataset['annotations']) # 443757\n",
    "# len(vqa.getQuesIds())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure\n",
    "```\n",
    "{\n",
    "\"info\" : info,\n",
    "\"task_type\" : str,\n",
    "\"data_type\": str,\n",
    "\"data_subtype\": str,\n",
    "\"questions\" : [question],\n",
    "\"license\" : license\n",
    "}\n",
    "\n",
    "info {\n",
    "\"year\" : int,\n",
    "\"version\" : str,\n",
    "\"description\" : str,\n",
    "\"contributor\" : str,\n",
    "\"url\" : str,\n",
    "\"date_created\" : datetime\n",
    "}\n",
    "\n",
    "license{\n",
    "\"name\" : str,\n",
    "\"url\" : str\n",
    "}\n",
    "\n",
    "question{\n",
    "\"question_id\" : int,\n",
    "\"image_id\" : int,\n",
    "\"question\" : str\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['info', 'task_type', 'data_type', 'license', 'data_subtype', 'questions']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vqa.questions.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `question` dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'image_id': 458752,\n",
       "  'question': 'What is this photo taken looking through?',\n",
       "  'question_id': 458752000},\n",
       " {'image_id': 458752,\n",
       "  'question': 'What position is this man playing?',\n",
       "  'question_id': 458752001},\n",
       " {'image_id': 458752,\n",
       "  'question': 'What color is the players shirt?',\n",
       "  'question_id': 458752002}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vqa.questions['questions'])[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing items in `question` dict by `question_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_id': 458752,\n",
       " 'question': 'What is this photo taken looking through?',\n",
       " 'question_id': 458752000}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vqa.qqa[458752000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotation\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure\n",
    "\n",
    "```\n",
    "{\n",
    "\"info\" : info,\n",
    "\"data_type\": str,\n",
    "\"data_subtype\": str,\n",
    "\"annotations\" : [annotation],\n",
    "\"license\" : license\n",
    "}\n",
    "\n",
    "info {\n",
    "\"year\" : int,\n",
    "\"version\" : str,\n",
    "\"description\" : str,\n",
    "\"contributor\" : str,\n",
    "\"url\" : str,\n",
    "\"date_created\" : datetime\n",
    "}\n",
    "\n",
    "license{\n",
    "\"name\" : str,\n",
    "\"url\" : str\n",
    "}\n",
    "\n",
    "annotation{\n",
    "\"question_id\" : int,\n",
    "\"image_id\" : int,\n",
    "\"question_type\" : str,\n",
    "\"answer_type\" : str,\n",
    "\"answers\" : [answer],\n",
    "\"multiple_choice_answer\" : str\n",
    "}\n",
    "\n",
    "answer{\n",
    "\"answer_id\" : int,\n",
    "\"answer\" : str,\n",
    "\"answer_confidence\": str\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['info', 'license', 'data_subtype', 'annotations', 'data_type']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vqa.dataset.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `annotation` dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question_type': 'what is this',\n",
       "  'multiple_choice_answer': 'net',\n",
       "  'answers': [{'answer': 'net', 'answer_confidence': 'maybe', 'answer_id': 1},\n",
       "   {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 2},\n",
       "   {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 3},\n",
       "   {'answer': 'netting', 'answer_confidence': 'yes', 'answer_id': 4},\n",
       "   {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 5},\n",
       "   {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 6},\n",
       "   {'answer': 'mesh', 'answer_confidence': 'maybe', 'answer_id': 7},\n",
       "   {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 8},\n",
       "   {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 9},\n",
       "   {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 10}],\n",
       "  'image_id': 458752,\n",
       "  'answer_type': 'other',\n",
       "  'question_id': 458752000},\n",
       " {'question_type': 'what',\n",
       "  'multiple_choice_answer': 'pitcher',\n",
       "  'answers': [{'answer': 'pitcher',\n",
       "    'answer_confidence': 'yes',\n",
       "    'answer_id': 1},\n",
       "   {'answer': 'catcher', 'answer_confidence': 'no', 'answer_id': 2},\n",
       "   {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 3},\n",
       "   {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 4},\n",
       "   {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 5},\n",
       "   {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 6},\n",
       "   {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 7},\n",
       "   {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 8},\n",
       "   {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 9},\n",
       "   {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 10}],\n",
       "  'image_id': 458752,\n",
       "  'answer_type': 'other',\n",
       "  'question_id': 458752001}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vqa.dataset['annotations'])[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing items in `annotation` dict by `question_id` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_type': 'what is this',\n",
       " 'multiple_choice_answer': 'net',\n",
       " 'answers': [{'answer': 'net', 'answer_confidence': 'maybe', 'answer_id': 1},\n",
       "  {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 2},\n",
       "  {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 3},\n",
       "  {'answer': 'netting', 'answer_confidence': 'yes', 'answer_id': 4},\n",
       "  {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 5},\n",
       "  {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 6},\n",
       "  {'answer': 'mesh', 'answer_confidence': 'maybe', 'answer_id': 7},\n",
       "  {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 8},\n",
       "  {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 9},\n",
       "  {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 10}],\n",
       " 'image_id': 458752,\n",
       " 'answer_type': 'other',\n",
       " 'question_id': 458752000}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vqa.qa[458752000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question_type': 'what is this',\n",
       "  'multiple_choice_answer': 'net',\n",
       "  'answers': [{'answer': 'net', 'answer_confidence': 'maybe', 'answer_id': 1},\n",
       "   {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 2},\n",
       "   {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 3},\n",
       "   {'answer': 'netting', 'answer_confidence': 'yes', 'answer_id': 4},\n",
       "   {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 5},\n",
       "   {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 6},\n",
       "   {'answer': 'mesh', 'answer_confidence': 'maybe', 'answer_id': 7},\n",
       "   {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 8},\n",
       "   {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 9},\n",
       "   {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 10}],\n",
       "  'image_id': 458752,\n",
       "  'answer_type': 'other',\n",
       "  'question_id': 458752000},\n",
       " {'question_type': 'what',\n",
       "  'multiple_choice_answer': 'pitcher',\n",
       "  'answers': [{'answer': 'pitcher',\n",
       "    'answer_confidence': 'yes',\n",
       "    'answer_id': 1},\n",
       "   {'answer': 'catcher', 'answer_confidence': 'no', 'answer_id': 2},\n",
       "   {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 3},\n",
       "   {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 4},\n",
       "   {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 5},\n",
       "   {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 6},\n",
       "   {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 7},\n",
       "   {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 8},\n",
       "   {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 9},\n",
       "   {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 10}],\n",
       "  'image_id': 458752,\n",
       "  'answer_type': 'other',\n",
       "  'question_id': 458752001},\n",
       " {'answer_type': 'other',\n",
       "  'multiple_choice_answer': 'orange',\n",
       "  'answers': [{'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 1},\n",
       "   {'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 2},\n",
       "   {'answer': 'orange', 'answer_confidence': 'maybe', 'answer_id': 3},\n",
       "   {'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 4},\n",
       "   {'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 5},\n",
       "   {'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 6},\n",
       "   {'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 7},\n",
       "   {'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 8},\n",
       "   {'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 9},\n",
       "   {'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 10}],\n",
       "  'image_id': 458752,\n",
       "  'question_type': 'what color is the',\n",
       "  'question_id': 458752002}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vqa.loadQA([458752000, 458752001, 458752002])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from build_vocab import *\n",
    "from vqaTools.vqa import VQA\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading VQA annotations and questions into memory...\n",
      "0:00:07.573418\n",
      "creating index...\n",
      "index created!\n",
      "[443757/443757] Tokenized the questions.\n",
      "Total vocabulary size: 7521\n",
      "Saved the vocabulary wrapper to '../../dotCuda/notebook/vocab.pkl'\n"
     ]
    }
   ],
   "source": [
    "rootDir = '../../data2'\n",
    "dataSubType = 'train2014'\n",
    "annFile ='{}/v2_mscoco_{}_annotations.json'.format(rootDir, dataSubType)\n",
    "quesFile ='{}/v2_OpenEnded_mscoco_{}_questions.json'.format(rootDir, dataSubType)\n",
    "\n",
    "vocab = build_vocab(annFile, quesFile, threshold=4)\n",
    "vocab_path = '../../dotCuda/notebook/vocab.pkl'\n",
    "\n",
    "with open(vocab_path, 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "print(\"Total vocabulary size: {}\".format(len(vocab)))\n",
    "print(\"Saved the vocabulary wrapper to '{}'\".format(vocab_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(vocab_path, 'rb') as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCODataset(Dataset):\n",
    "    \n",
    "    def __init__(self, vocab, rootDir='../../data2/', dataSubType='train2014', transform=None):\n",
    "        \n",
    "        annFile ='{}/v2_mscoco_{}_annotations.json'.format(rootDir, dataSubType)\n",
    "        quesFile ='{}/v2_OpenEnded_mscoco_{}_questions.json'.format(rootDir, dataSubType)\n",
    "        self.vqa = VQA(annFile, quesFile)\n",
    "        self.imgDir = '{}/{}'.format(rootDir, dataSubType)\n",
    "        self.vocab = vocab\n",
    "        self.quesIds = self.vqa.getQuesIds()\n",
    "        self.dataSubType = dataSubType\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        \"\"\"\n",
    "        returns:\n",
    "            actual question as tensor of word-indices\n",
    "            image_id\n",
    "            indices of answers mapped to 3000 most frequent answers ?\n",
    "        \"\"\"\n",
    "        \n",
    "        quesId = self.quesIds[index]\n",
    "        \n",
    "        img_id = self.vqa.qqa[quesId]['image_id']        \n",
    "        path = 'COCO_{}_000000{}.jpg'.format(self.dataSubType, img_id)\n",
    "        image = Image.open(os.path.join(self.imgDir, path)).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "            \n",
    "        # Convert question to word ids\n",
    "        vocab = self.vocab\n",
    "        question = self.vqa.qqa[quesId]['question']\n",
    "        print(question)\n",
    "        tokens = nltk.tokenize.word_tokenize(question.lower())\n",
    "        question_list = []\n",
    "        question_list.append(vocab('<start>'))\n",
    "        question_list.extend([vocab(token) for token in tokens])\n",
    "        question_list.append(vocab('<end>'))\n",
    "        question_tensor = torch.Tensor(question_list)\n",
    "        \n",
    "        qa = self.vqa.loadQA(quesId)\n",
    "        \n",
    "        ans_list = [a['answer'] for a in qa[0]['answers']] # ?\n",
    "\n",
    "        return question_tensor, image, ans_list\n",
    "        \n",
    "        \n",
    "    def __len__():\n",
    "        return len(self.vqa.dataset['annotations'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading VQA annotations and questions into memory...\n",
      "0:00:07.472525\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "dataset = COCODataset(vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is this photo taken looking through?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([  1.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,  11.,   2.]),\n",
       " <PIL.Image.Image image mode=RGB size=640x480 at 0x7F5938DFADD8>,\n",
       " ['net', 'net', 'net', 'netting', 'net', 'net', 'mesh', 'net', 'net', 'net'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:AI_Proj]",
   "language": "python",
   "name": "conda-env-AI_Proj-py"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "326px",
    "left": "951.333px",
    "right": "20px",
    "top": "70px",
    "width": "394px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
