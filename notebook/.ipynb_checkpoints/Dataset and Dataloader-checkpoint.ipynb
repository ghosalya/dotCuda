{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data folder structure\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['v2_mscoco_val2014_complementary_pairs.json',\n",
       " 'v2_Questions_Val_mscoco.zip',\n",
       " 'v2_mscoco_val2014_annotations.json',\n",
       " 'v2_OpenEnded_mscoco_val2014_questions.json',\n",
       " 'v2_mscoco_train2014_annotations.json',\n",
       " 'annotations',\n",
       " 'train2014',\n",
       " 'v2_OpenEnded_mscoco_train2014_questions.json',\n",
       " 'v2_Questions_Train_mscoco.zip',\n",
       " 'v2_mscoco_train2014_complementary_pairs.json',\n",
       " 'val2014']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDir ='../../data2'\n",
    "os.listdir(dataDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes on data folder\n",
    "--\n",
    "- `annotations`: not required, only for image captioning\n",
    "- `train2014` and `val2014`: img folders with format e.g. `COCO_val2014_000000059710.jpg`\n",
    "- `v2_Questions_Train_mscoco.zip` and `v2_Questions_Val_mscoco.zip`: unzip into respective questions `json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from vqaTools.vqa import VQA\n",
    "\n",
    "dataDir ='../../data2'\n",
    "versionType ='v2_' # this should be '' when using VQA v2.0 dataset\n",
    "taskType ='OpenEnded' # 'OpenEnded' only for v2.0. 'OpenEnded' or 'MultipleChoice' for v1.0\n",
    "dataType ='mscoco'  # 'mscoco' only for v1.0. 'mscoco' for real and 'abstract_v002' for abstract for v1.0.\n",
    "dataSubType ='train2014'\n",
    "annFile ='{}/{}{}_{}_annotations.json'.format(dataDir, versionType, dataType, dataSubType)\n",
    "quesFile ='{}/{}{}_{}_{}_questions.json'.format(dataDir, versionType, taskType, dataType, dataSubType)\n",
    "imgDir = '{}/{}/'.format(dataDir, dataSubType)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading VQA annotations and questions into memory...\n",
      "0:00:10.382276\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# initialize VQA api for QA annotations\n",
    "vqa = VQA(annFile, quesFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'answer_type': 'other',\n",
       "  'multiple_choice_answer': 'orange',\n",
       "  'answers': [{'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 1},\n",
       "   {'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 2},\n",
       "   {'answer': 'orange', 'answer_confidence': 'maybe', 'answer_id': 3},\n",
       "   {'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 4},\n",
       "   {'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 5},\n",
       "   {'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 6},\n",
       "   {'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 7},\n",
       "   {'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 8},\n",
       "   {'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 9},\n",
       "   {'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 10}],\n",
       "  'image_id': 458752,\n",
       "  'question_type': 'what color is the',\n",
       "  'question_id': 458752002}]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "image = vqa.qqa[458752000]['image_id']\n",
    "qa = vqa.loadQA(458752002)\n",
    "# qa['multiple_choice_answer']\n",
    "# [answer for answer in qa[0]['answers']\n",
    "\n",
    "qa\n",
    " \n",
    "# for a in qa[0]['answers']:\n",
    "#     print(a['answer'])\n",
    "# ans_list = [a['answer'] for a in qa[0]['answers']]\n",
    "\n",
    "\n",
    "# len(vqa.dataset['annotations']) # 443757\n",
    "# len(vqa.getQuesIds())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure\n",
    "```\n",
    "{\n",
    "\"info\" : info,\n",
    "\"task_type\" : str,\n",
    "\"data_type\": str,\n",
    "\"data_subtype\": str,\n",
    "\"questions\" : [question],\n",
    "\"license\" : license\n",
    "}\n",
    "\n",
    "info {\n",
    "\"year\" : int,\n",
    "\"version\" : str,\n",
    "\"description\" : str,\n",
    "\"contributor\" : str,\n",
    "\"url\" : str,\n",
    "\"date_created\" : datetime\n",
    "}\n",
    "\n",
    "license{\n",
    "\"name\" : str,\n",
    "\"url\" : str\n",
    "}\n",
    "\n",
    "question{\n",
    "\"question_id\" : int,\n",
    "\"image_id\" : int,\n",
    "\"question\" : str\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['info', 'task_type', 'data_type', 'license', 'data_subtype', 'questions']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vqa.questions.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `question` dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'image_id': 458752,\n",
       "  'question': 'What is this photo taken looking through?',\n",
       "  'question_id': 458752000},\n",
       " {'image_id': 458752,\n",
       "  'question': 'What position is this man playing?',\n",
       "  'question_id': 458752001},\n",
       " {'image_id': 458752,\n",
       "  'question': 'What color is the players shirt?',\n",
       "  'question_id': 458752002}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vqa.questions['questions'])[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing items in `question` dict by `question_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_id': 458752,\n",
       " 'question': 'What is this photo taken looking through?',\n",
       " 'question_id': 458752000}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vqa.qqa[458752000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotation\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure\n",
    "\n",
    "```\n",
    "{\n",
    "\"info\" : info,\n",
    "\"data_type\": str,\n",
    "\"data_subtype\": str,\n",
    "\"annotations\" : [annotation],\n",
    "\"license\" : license\n",
    "}\n",
    "\n",
    "info {\n",
    "\"year\" : int,\n",
    "\"version\" : str,\n",
    "\"description\" : str,\n",
    "\"contributor\" : str,\n",
    "\"url\" : str,\n",
    "\"date_created\" : datetime\n",
    "}\n",
    "\n",
    "license{\n",
    "\"name\" : str,\n",
    "\"url\" : str\n",
    "}\n",
    "\n",
    "annotation{\n",
    "\"question_id\" : int,\n",
    "\"image_id\" : int,\n",
    "\"question_type\" : str,\n",
    "\"answer_type\" : str,\n",
    "\"answers\" : [answer],\n",
    "\"multiple_choice_answer\" : str\n",
    "}\n",
    "\n",
    "answer{\n",
    "\"answer_id\" : int,\n",
    "\"answer\" : str,\n",
    "\"answer_confidence\": str\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['info', 'license', 'data_subtype', 'annotations', 'data_type']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vqa.dataset.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `annotation` dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question_type': 'what is this',\n",
       "  'multiple_choice_answer': 'net',\n",
       "  'answers': [{'answer': 'net', 'answer_confidence': 'maybe', 'answer_id': 1},\n",
       "   {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 2},\n",
       "   {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 3},\n",
       "   {'answer': 'netting', 'answer_confidence': 'yes', 'answer_id': 4},\n",
       "   {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 5},\n",
       "   {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 6},\n",
       "   {'answer': 'mesh', 'answer_confidence': 'maybe', 'answer_id': 7},\n",
       "   {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 8},\n",
       "   {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 9},\n",
       "   {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 10}],\n",
       "  'image_id': 458752,\n",
       "  'answer_type': 'other',\n",
       "  'question_id': 458752000},\n",
       " {'question_type': 'what',\n",
       "  'multiple_choice_answer': 'pitcher',\n",
       "  'answers': [{'answer': 'pitcher',\n",
       "    'answer_confidence': 'yes',\n",
       "    'answer_id': 1},\n",
       "   {'answer': 'catcher', 'answer_confidence': 'no', 'answer_id': 2},\n",
       "   {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 3},\n",
       "   {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 4},\n",
       "   {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 5},\n",
       "   {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 6},\n",
       "   {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 7},\n",
       "   {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 8},\n",
       "   {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 9},\n",
       "   {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 10}],\n",
       "  'image_id': 458752,\n",
       "  'answer_type': 'other',\n",
       "  'question_id': 458752001}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vqa.dataset['annotations'])[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing items in `annotation` dict by `question_id` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_type': 'what is this',\n",
       " 'multiple_choice_answer': 'net',\n",
       " 'answers': [{'answer': 'net', 'answer_confidence': 'maybe', 'answer_id': 1},\n",
       "  {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 2},\n",
       "  {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 3},\n",
       "  {'answer': 'netting', 'answer_confidence': 'yes', 'answer_id': 4},\n",
       "  {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 5},\n",
       "  {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 6},\n",
       "  {'answer': 'mesh', 'answer_confidence': 'maybe', 'answer_id': 7},\n",
       "  {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 8},\n",
       "  {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 9},\n",
       "  {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 10}],\n",
       " 'image_id': 458752,\n",
       " 'answer_type': 'other',\n",
       " 'question_id': 458752000}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vqa.qa[458752000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-08T19:01:23.087469Z",
     "start_time": "2018-07-08T19:01:23.080596Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question_type': 'is this',\n",
       "  'multiple_choice_answer': 'yes',\n",
       "  'answers': [{'answer': 'yes', 'answer_confidence': 'yes', 'answer_id': 1},\n",
       "   {'answer': 'yes', 'answer_confidence': 'yes', 'answer_id': 2},\n",
       "   {'answer': 'yes', 'answer_confidence': 'maybe', 'answer_id': 3},\n",
       "   {'answer': 'yes', 'answer_confidence': 'yes', 'answer_id': 4},\n",
       "   {'answer': 'yes', 'answer_confidence': 'maybe', 'answer_id': 5},\n",
       "   {'answer': 'no', 'answer_confidence': 'maybe', 'answer_id': 6},\n",
       "   {'answer': 'yes', 'answer_confidence': 'yes', 'answer_id': 7},\n",
       "   {'answer': 'yes', 'answer_confidence': 'yes', 'answer_id': 8},\n",
       "   {'answer': 'yes', 'answer_confidence': 'yes', 'answer_id': 9},\n",
       "   {'answer': 'yes', 'answer_confidence': 'maybe', 'answer_id': 10}],\n",
       "  'image_id': 458752,\n",
       "  'answer_type': 'yes/no',\n",
       "  'question_id': 458752003},\n",
       " {'answer_type': 'other',\n",
       "  'multiple_choice_answer': 'orange',\n",
       "  'answers': [{'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 1},\n",
       "   {'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 2},\n",
       "   {'answer': 'orange', 'answer_confidence': 'maybe', 'answer_id': 3},\n",
       "   {'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 4},\n",
       "   {'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 5},\n",
       "   {'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 6},\n",
       "   {'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 7},\n",
       "   {'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 8},\n",
       "   {'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 9},\n",
       "   {'answer': 'orange', 'answer_confidence': 'yes', 'answer_id': 10}],\n",
       "  'image_id': 458752,\n",
       "  'question_type': 'what color is the',\n",
       "  'question_id': 458752002},\n",
       " {'question_type': 'what',\n",
       "  'multiple_choice_answer': 'pitcher',\n",
       "  'answers': [{'answer': 'pitcher',\n",
       "    'answer_confidence': 'yes',\n",
       "    'answer_id': 1},\n",
       "   {'answer': 'catcher', 'answer_confidence': 'no', 'answer_id': 2},\n",
       "   {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 3},\n",
       "   {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 4},\n",
       "   {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 5},\n",
       "   {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 6},\n",
       "   {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 7},\n",
       "   {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 8},\n",
       "   {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 9},\n",
       "   {'answer': 'pitcher', 'answer_confidence': 'yes', 'answer_id': 10}],\n",
       "  'image_id': 458752,\n",
       "  'answer_type': 'other',\n",
       "  'question_id': 458752001}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vqa.loadQA([458752003, 458752002, 458752001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Creating the Dataset\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T09:24:51.949295Z",
     "start_time": "2018-07-09T09:24:50.415229Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from build_vocab import *\n",
    "from build_answers import *\n",
    "from vqaTools.vqa import VQA\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T09:24:51.960346Z",
     "start_time": "2018-07-09T09:24:51.957023Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rootDir = '../../data2'\n",
    "dataSubType = 'train2014'\n",
    "annFile ='{}/v2_mscoco_{}_annotations.json'.format(rootDir, dataSubType)\n",
    "quesFile ='{}/v2_OpenEnded_mscoco_{}_questions.json'.format(rootDir, dataSubType)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build an answers wrapper\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T09:25:19.463048Z",
     "start_time": "2018-07-09T09:25:06.313116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading VQA annotations and questions into memory...\n",
      "0:00:10.742081\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# instantiate VQA object\n",
    "vqa = VQA(annFile, quesFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T09:25:55.819339Z",
     "start_time": "2018-07-09T09:25:40.567800Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading VQA annotations and questions into memory...\n",
      "0:00:08.353778\n",
      "creating index...\n",
      "index created!\n",
      "len of annotations dict: 443757\n",
      "[443757/443757] Answers tally completed.\n"
     ]
    }
   ],
   "source": [
    "answers = build_answers(annFile, quesFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T09:25:55.828593Z",
     "start_time": "2018-07-09T09:25:55.826047Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answers_path = '../../dotCuda/notebook/answers.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T09:26:00.814010Z",
     "start_time": "2018-07-09T09:26:00.808676Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total answers size: 3000\n",
      "Saved the answers wrapper to '../../dotCuda/notebook/answers.pkl'\n"
     ]
    }
   ],
   "source": [
    "with open(answers_path, 'wb') as f:\n",
    "    pickle.dump(answers, f)\n",
    "print(\"Total answers size: {}\".format(len(answers)))\n",
    "print(\"Saved the answers wrapper to '{}'\".format(answers_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T09:26:02.297487Z",
     "start_time": "2018-07-09T09:26:02.287301Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(answers_path, 'rb') as f:\n",
    "    answers = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Build a vocabulary wrapper\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T09:27:06.676627Z",
     "start_time": "2018-07-09T09:26:05.470427Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading VQA annotations and questions into memory...\n",
      "0:00:06.861099\n",
      "creating index...\n",
      "index created!\n",
      "[443757/443757] Tokenized the questions.\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(annFile, quesFile, threshold=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T09:27:10.212241Z",
     "start_time": "2018-07-09T09:27:10.209186Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_path = '../../dotCuda/notebook/vocab.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T09:27:11.696159Z",
     "start_time": "2018-07-09T09:27:11.689108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary size: 7521\n",
      "Saved the vocabulary wrapper to '../../dotCuda/notebook/vocab.pkl'\n"
     ]
    }
   ],
   "source": [
    "with open(vocab_path, 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "print(\"Total vocabulary size: {}\".format(len(vocab)))\n",
    "print(\"Saved the vocabulary wrapper to '{}'\".format(vocab_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T09:27:17.485981Z",
     "start_time": "2018-07-09T09:27:17.479244Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(vocab_path, 'rb') as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the image\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T09:27:19.087931Z",
     "start_time": "2018-07-09T09:27:19.083919Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T09:27:35.652621Z",
     "start_time": "2018-07-09T09:27:35.644059Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class COCODataset(Dataset):\n",
    "    \n",
    "    def __init__(self, vocab, answers, rootDir='../../data2/', dataSubType='train2014', transform=transform):\n",
    "        \n",
    "        annFile ='{}/v2_mscoco_{}_annotations.json'.format(rootDir, dataSubType)\n",
    "        quesFile ='{}/v2_OpenEnded_mscoco_{}_questions.json'.format(rootDir, dataSubType)\n",
    "        self.vqa = VQA(annFile, quesFile)\n",
    "        self.imgDir = '{}/{}'.format(rootDir, dataSubType)\n",
    "        self.vocab = vocab\n",
    "        self.answers = answers\n",
    "        self.quesIds = self.vqa.getQuesIds()\n",
    "        self.dataSubType = dataSubType\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        \"\"\"\n",
    "        returns:\n",
    "            question tensor of word-indices\n",
    "            transformed image tensor: [3, 224, 224]\n",
    "            answers tensor of indices mapped to 3000 most frequently occurring answers: [no. of answers]\n",
    "            answers with not found among 300 most frequently occurring answers are eliminated\n",
    "        \"\"\"\n",
    "        \n",
    "        quesId = self.quesIds[index]\n",
    "        \n",
    "        img_id = self.vqa.qqa[quesId]['image_id']        \n",
    "        path = 'COCO_{}_000000{}.jpg'.format(self.dataSubType, img_id)\n",
    "        image = Image.open(os.path.join(self.imgDir, path)).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "            \n",
    "        # Convert question to word ids\n",
    "        vocab = self.vocab\n",
    "        question = self.vqa.qqa[quesId]['question']\n",
    "        print(question)\n",
    "        \n",
    "        tokens = nltk.tokenize.word_tokenize(question.lower())\n",
    "        question_list = []\n",
    "        question_list.append(vocab('<start>'))\n",
    "        question_list.extend([vocab(token) for token in tokens])\n",
    "        question_list.append(vocab('<end>'))\n",
    "        question_tensor = torch.Tensor(question_list)\n",
    "        \n",
    "        qa = self.vqa.loadQA(quesId)\n",
    "        \n",
    "        ans_list = [a['answer'] for a in qa[0]['answers']]\n",
    "        print(ans_list)\n",
    "        \n",
    "        ans_index_list = [self.answers.ans2idx[ans] for ans in ans_list if ans in self.answers.ans2idx.keys()]\n",
    "        answer_tensor = torch.Tensor(ans_index_list)\n",
    "        \n",
    "        return question_tensor, image, answer_tensor\n",
    "        \n",
    "        \n",
    "    def __len__():\n",
    "        return len(self.vqa.dataset['annotations'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T09:27:50.283718Z",
     "start_time": "2018-07-09T09:27:40.434419Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading VQA annotations and questions into memory...\n",
      "0:00:06.836632\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "dataset = COCODataset(vocab=vocab, answers=answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T09:28:06.385577Z",
     "start_time": "2018-07-09T09:28:06.260333Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is this photo taken looking through?\n",
      "['net', 'net', 'net', 'netting', 'net', 'net', 'mesh', 'net', 'net', 'net']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([  1.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,  11.,   2.]),\n",
       " tensor([[[ 0.2282,  0.2453,  0.3138,  ...,  0.9474,  0.8618,  0.9988],\n",
       "          [ 0.0741,  0.1083,  0.2111,  ...,  1.0331,  0.9474,  1.0673],\n",
       "          [ 0.4851,  0.4508,  0.5193,  ...,  1.0159,  0.9303,  1.0502],\n",
       "          ...,\n",
       "          [-0.1143, -0.2684, -0.1999,  ..., -1.1589, -1.1418, -1.1418],\n",
       "          [-0.2856, -0.1486, -0.1143,  ..., -1.2445, -1.2788, -1.1760],\n",
       "          [-0.1828, -0.3198, -0.1999,  ..., -1.2103, -1.2274, -1.2103]],\n",
       " \n",
       "         [[ 0.5203,  0.5378,  0.6429,  ...,  0.5903,  0.5728,  0.7129],\n",
       "          [ 0.2752,  0.3452,  0.3978,  ...,  0.7129,  0.6954,  0.8354],\n",
       "          [ 0.3803,  0.4153,  0.5378,  ...,  0.7129,  0.6954,  0.8354],\n",
       "          ...,\n",
       "          [-0.3725, -0.5476, -0.4951,  ..., -1.0378, -1.0378, -1.0553],\n",
       "          [-0.4951, -0.3550, -0.3200,  ..., -1.1078, -1.1604, -1.0903],\n",
       "          [-0.3725, -0.5126, -0.4076,  ..., -1.1078, -1.1604, -1.2129]],\n",
       " \n",
       "         [[ 0.0082,  0.0256,  0.0082,  ...,  0.2871,  0.1999,  0.2871],\n",
       "          [-0.1661, -0.1661, -0.0615,  ...,  0.3568,  0.2871,  0.3742],\n",
       "          [-0.0964,  0.0431,  0.1476,  ...,  0.3219,  0.2871,  0.4439],\n",
       "          ...,\n",
       "          [-0.3404, -0.5147, -0.3753,  ..., -0.9156, -0.9156, -0.9156],\n",
       "          [-0.5495, -0.4101, -0.3055,  ..., -0.9504, -0.9853, -0.8981],\n",
       "          [-0.4101, -0.4798, -0.3753,  ..., -0.8981, -0.9156, -0.9504]]]),\n",
       " tensor([  903.,   903.,   903.,   903.,   903.,  2852.,   903.,   903.,\n",
       "           903.]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T09:32:42.769307Z",
     "start_time": "2018-07-09T09:32:42.746773Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is this photo taken looking through?\n",
      "['net', 'net', 'net', 'netting', 'net', 'net', 'mesh', 'net', 'net', 'net']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([9])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_tensor, image, ans_index_list = dataset[0]\n",
    "ans_index_list.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Dataloader\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:AI_Proj]",
   "language": "python",
   "name": "conda-env-AI_Proj-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "326px",
    "left": "951.333px",
    "right": "20px",
    "top": "70px",
    "width": "394px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
